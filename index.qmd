---
title: "Cross-Validation - Data Science Capstone"
author: "Jolie Wise"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is Cross-Validation?

This is an introduction to cross-validation...

### Related work

Lei (2020) [@lei2020] begins by explaining that cross-validation is to create and evaluate models based on split data sets to limit bias when testing a model. The traditional cross-validation methods, including leave-one-out and V-fold, can be prone to overfitting due to the standard small split ratio. The smaller training set creates an oversimplified model that fails to account for uncertainty in the testing set. This article creates what they call cross-validation with confidence (CVC) that intends to address these issues. It compares the predictive risk of each candidate model, and it finds the p-values of the residuals of each candidate model to pick out the models where the null hypothesis is not rejected. CVC can be used for model selection, as well as parameter tuning. The traditional methods for cross-validation still come into play, but CVC requires some extra computational complexity once the p-values for every candidate model or set of parameters are computed; however, the article claims the total running time is comparable to other model selection methods. CVC was found to stand up against other linear model selection methods when the sample size is moderately large and the true model has a lower number of predictors. The article compares the performance of CVC in tuning parameters of Lasso regularization to the performance of standard cross-validation, the estimation-stability method, and the 1-standard-error rule. CVC was competitive with these methods, as it was able to provide a predictive model with fewer predictors while still being easier to interpret. The article also acknowledges that there is more work to be done on this method. For example, the method has yet to consider unsupervised methods, and binary response variables require other considerations from what was tested in the article.

Rabinowicz & Rosset (2022) [@rabinowicz2022] intend to address how correlation structure affects cross-validation and introduce a measure that corrects for bias in cross-validation (CVc). Correction to cross-validation is not needed when splitting the dataset does not change the distributional relationship between training and testing sets. In this scenario, cross-validation is unbiased, as well as when the weights are zero. The article also presents a few examples of more complex correlational structures. One of which was Kriging for spatial and temporal data, and another was longitudinal data where the same individuals are studied and observed over time. CVc was compared to other variations of cross-validation, such as leave cluster out (LCO), leave observation from each cluster out (LOFCO), and h-blocking. It is acknowledged that these variations apply to specific kinds of datasets for any modeling method. Whereas the proposed measure, CVc, applies to linear models using a wide range of data set types. This measure is applied to two real datasets, one with a cluster correlation structure and one with a hierarchical spatial correlation structure. Its performance is compared to that of standard cross-validation. In both examples, the bias-correction measure estimates the generalization error better than standard cross-validation. Also, as long as the covariance structure is specified, the proposed measure will remain unbiased even if the appropriate prediction method is not used. In this case, they used generalized least squares (GLS) and linear mixed models (LMM).

## Methods

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

## References
