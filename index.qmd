---
title: "Cross-Validation - Data Science Capstone"
author: "Jolie Wise"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is Cross-Validation?

This is an introduction to cross-validation...

### Related work

Lei (2020) [@lei2020] begins by explaining that cross-validation is to create and evaluate models based on split data sets to limit bias when testing a model. The traditional cross-validation methods, including leave-one-out and V-fold, can be prone to overfitting due to the standard small split ratio. The smaller training set creates an oversimplified model that fails to account for uncertainty in the testing set. This article creates what they call cross-validation with confidence (CVC) that intends to address these issues. It compares the predictive risk of each candidate model, and it finds the p-values of the residuals of each candidate model to pick out the models where the null hypothesis is not rejected. CVC can be used for model selection, as well as parameter tuning. The traditional methods for cross-validation still come into play, but CVC requires some extra computational complexity once the p-values for every candidate model or set of parameters are computed; however, the article claims the total running time is comparable to other model selection methods. CVC was found to stand up against other linear model selection methods when the sample size is moderately large and the true model has a lower number of predictors. The article compares the performance of CVC in tuning parameters of Lasso regularization to the performance of standard cross-validation, the estimation-stability method, and the 1-standard-error rule. CVC was competitive with these methods, as it was able to provide a predictive model with fewer predictors while still being easier to interpret. The article also acknowledges that there is more work to be done on this method. For example, the method has yet to consider unsupervised methods, and binary response variables require other considerations from what was tested in the article.

Rabinowicz & Rosset (2022) [@rabinowicz2022] intend to address how correlation structure affects cross-validation and introduce a measure that corrects for bias in cross-validation (CVc). Correction to cross-validation is not needed when splitting the dataset does not change the distributional relationship between training and testing sets. In this scenario, cross-validation is unbiased, as well as when the weights are zero. The article also presents a few examples of more complex correlational structures. One of which was Kriging for spatial and temporal data, and another was longitudinal data where the same individuals are studied and observed over time. CVc was compared to other variations of cross-validation, such as leave cluster out (LCO), leave observation from each cluster out (LOFCO), and h-blocking. It is acknowledged that these variations apply to specific kinds of datasets for any modeling method. Whereas the proposed measure, CVc, applies to linear models using a wide range of data set types. This measure is applied to two real datasets, one with a cluster correlation structure and one with a hierarchical spatial correlation structure. Its performance is compared to that of standard cross-validation. In both examples, the bias-correction measure estimates the generalization error better than standard cross-validation. Also, as long as the covariance structure is specified, the proposed measure will remain unbiased even if the appropriate prediction method is not used. In this case, they used generalized least squares (GLS) and linear mixed models (LMM).

Zhang and Yang (2015) [@zhangyang2015] focus on cross-validation as a model selection method. The basic idea is that the data is split so that one portion can be used to fit all potential models, and the rest of the data is used to evaluate performance. The article points out the different approaches to cross-validation that determine the consistency of model selection, especially in high-dimensional settings. The debate between using Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) as model selection methods is referenced as well. The decision of which to use is summarized as whether the data requires parametric or non-parametric methods. The article claims that cross-validation will select the appropriate criterion with proper data splitting. It is acknowledged that there have been other methods to solve this problem by combining the criterion. Zhang and Yang also discuss how to select an appropriate data-splitting ratio. For example, for models where the number of features approaches infinity, any fixed ratio is appropriate theoretically. A few different simulations are done to highlight the difference the data-splitting ratio can make in cross-validation performance. In the parametric scenario, a larger testing set lessened cross-validation errors, but parameter estimate errors increased when the size of the training set fell below a certain threshold. In the non-parametric scenario, the half-half ratio performed the best. The article also mentions a few misconceptions about cross-validations. One of which is a better estimation of prediction error indicates better model selection.

Wong and Yeh (2020) [@wongyeh2020] consider the effect of repeatedly performing k-fold cross-validation on the accuracy of classification algorithms. The article suggests different ways to calculate variance, as a relatively small variance indicates a more reliable accuracy estimate. The different variance variance estimates are evaluated based on estimation stability and sensitivity to the mean estimate. K-nearest neighbors with k=1 (1NN) was used to explore the effect of replications on accuracy estimates. Wong and Yeh found that the probability of an instance in two replications is dependent, as the number of instances decreases as the number of folds increases. The article includes an experiment to compare the effect of the number of folds on the normalized dependency of accuracy estimates between two different classification algorithms, a support vector machine (SVM) and the NaÄ±ve Bayesian classifier (NBC). Twenty different datasets were used, with a varying number of instances and features. The number of folds tested was two, five, and ten, as these are the typical number of folds used. Both algorithms demonstrated a higher number of significant scores as the folds increased. Ten-fold cross-validation produced the highest number of significant scores and should be considered the first choice for k-fold cross-validation. The article claims for scenarios where 10-fold cross-validation violates large-sample conditions that the next choice should be to repeat 5-fold cross-validation twice.

## Methods

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

## References
